---
Slides: https://drive.google.com/file/d/1sfY_iHkNDvrUvoPolPQnl1Z4utSOL6o8/view
Recording: https://www.youtube.com/watch?v=FoKMxugH4hI&feature=youtu.be
---
https://youtu.be/FoKMxugH4hI?si=ahd2_Mrx_76k8PR9&t=190

# Cloud Storage (i.e. AWS)
3 types
1. **==object store==** (AWS S3, GCP cloud storage, Dropbox, backups)
	- why they use chain replication?  large objects [[Lecture 7 (10-21-2024)#Replication]]
		- 
	- tons of unstructured data, flat (unlike hierarchy in normal file sys)
		- simulate directory with "prefix" to filename in bucket, some clients recognize as folder
	- for **data lakes** diverse data, long term data storage (archival logs, static assets for website)
	- not attached to single server or disk, standalone, flexible, for AWS logs
	- each file organized in **buckets** w/ unique name (across ALL of AWS not region)
	- buckets replicated **among** regions and **within** regions
		- among: spread across multiple regions
		- within: availability zone in case of geographic outage/locality in data center 
	- tradeoff/tiers
		- high perf low latency \$\$\$\$ (expensive to store)
		- infrequently accessed (move inactive users, more expensive to retrieval)
		- archival (rarely used data, dirt cheap, \$\$\$\$ retrieve i.e. in cave super slow)
		- [[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg|Open: Pasted image 20241123000036.png]]![[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg]]
	- nickel-diming pricing (always price to retrieve/egress data, free upload/ingress)
		- region (LA \$\$\$)
		- data (GBs/month, discounts)
		- num requests (every 1k PUT/COPY/POST/WRITE,  GET/SELECT)
		- storage class changes (every 1k) - auto intelligient tiers also \$\$\$
		- replication
		- Examples
			- store 50GB in standard, 100k files. Egress=500GB (i.e. retrieve 10 times, 1 million reads)
			- so: writes=100k writes. storage: 50GB in standard, standard pricing=.023/GB and .005/1k writes
			- ingress: 100k writes / 1k x $.005 + 50 GB(\$.023) = 1.65
			- egress: $.09(500GB) + 1M reads / 1k x \$.0004 = 45.4
			- total: $1.65+$45.4 = $47.05
		- lots of small files but total lots of GB = tons of writes, reads, \$\$\$\$
3. **==block lvl==** store (AWS EBS, GCP persistent disk)
	- like disk in computer, attached to server instance, needs mounting, partition, formatting
	- way faster than object storage like real SSD
	- pricing: EC2 server runtime + storing/reading OS installation into the EBS
	- classes w/ sub-versions (eras)
		- general purpose (SSD), practical, cost effective, IOPS not guaranteed
		- provisioned IOPS (faster, IOPS=I/Os per sec) - databases, production web servers
		- throughput optimized hdd - optimized for high thruput i.e. WD Purple
		- cold hdd = commodity hdd slow, cheap
	- pricing - even if unmounted or not associated with instance
		- region
		- storage class
		- num provisioned IOPS above baseline
		- thruput MB/s
		- i.e. small files = greater thruput and IOPS
4. **==cloud/distrib. file sys==** (AWS EFS, GCP Filestore)
	- like EBS is disk needing mounting, but supports multiple EC2 server instance connections not just one
	- ideally EC2 server and EBS disk in same data center
	- unlike SSD partitions + file sys, EFS model completely independent of disks
		- simply shared file system abstract away underlying disk vols
	- classes
		- tiering = automatic placement/optimizing of which tier, decided per file
		[[e19e6383cf5eab3dcfc205d8931aad0e_MD5.jpeg|Open: Pasted image 20241123003030.png]]
![[e19e6383cf5eab3dcfc205d8931aad0e_MD5.jpeg]]
# Consensus - hides replication from user
replication and coordination failures
- GFS: master failure fine just heartbeat chunkservers for what they contain + operation log snapshot for location of chunkservers
- chain replication: head(write coordinator) and tail (read coord)
	- middle node dies: remove from linked list
	- head node dies: make next node w/ already replicated data new head (easy) or elect new head
- primary-backup replication?
	- harder than chain bc async network, multiple new primary candidates
	- options
		- system rejects reads/writes in case primary recovers
		- choose new primary, but missing metadata like in chain
		- small group of backup coordinators but must act as one
what is consensus: like ML classifier
- [[d6451a0ca909906c113b58141c9dfb68_MD5.jpeg|Open: Pasted image 20241123005945.png]]
![[d6451a0ca909906c113b58141c9dfb68_MD5.jpeg]] here using majority consensus rule
- all processes need to agree on a bit w/ their limited POV
- can only satisfy TWO of these properties:
	- **termination**: each correct (not crashed) process **eventually** decides on some val
	- **agreement**: all decide on same value
	- **validity**: agreed value is correct from consensus role
- so compromise on termination. 
when is **consensus** needed? extremely difficult on async. 
- ensure same order - ordered broadcast problem
- all processes know each other - group membership problem
- one process particular special role and others know - leader election
- take turns on shared resource - distrib. mutual exclusion
- agree commit/abort/rollback decision - distrib. transactions
# Paxos
- story: ficticious ancient civilization on real island, so had part-time parliament gov to do it, not always there but have consensus on laws
- 2001 version dropped the story, only keeps algorithm 
- uses majority wins rule, hangs if majority processes not available
- To tolerate m process failures, Paxos needs a total of 2m + 1 processes.
- if I ask majority if anyone proposed party location and they say no, is true
- process roles
	- proposer
	- acceptor
	- learner: execut
- paxos nodes: 
	- must persist data
	- and know how many = majority (m if total 2m+1)
- paxos phases
	- **prepare** - establish latest generation using proposal num (to save IO in case ignore)
		- proposer sends Prep(n) msg to at least majority
		- acceptors only accept proposal nums >= already seen and reply with Promise(n) else ignore
		- phase passes if majority of acceptors respond with promise
	- **accept**+commit - propose value and let everyone know
		- proposer: Accept(n, val) to least majority 
		- acceptor: Accepted(n, val) if not largest val, forward to all learners
- simplified example
	- [[1bc341f1e7f8bce2dc61a17ea35649f8_MD5.jpeg|Open: Pasted image 20241123024455.png]]
![[1bc341f1e7f8bce2dc61a17ea35649f8_MD5.jpeg]]
- more than one proposer? (simplify by omitting learners)
	- phase 1 change: when acceptor receives Promise(5) but already accepted a proposal, send Promise(5, (accepted_n, accepted_val)) otherwise just Promise(5)
	- phase 2 change: 
		- proposer: get latest already accepted val from all responding acceptors and admit defeat, otherwise Accept(n,val) as before. 
		- acceptor: needs to store n and val of newly accepted proposal (persistent state) 
	- A3 doesn't see 5 so accepts 4, but A2 saw 5 and rejects 4
	- P1 goes through normal phase 2
	- P2 goes through normal phase 1 with new even proposal num, 6
	- P2 takes the latest accepted value (foo) and asks to accept that [[cf381600e1bfb70ba51719f438f221ac_MD5.jpeg|]]
![[cf381600e1bfb70ba51719f438f221ac_MD5.jpeg]]
- **==Dueling proposers==**
	![[8c6fd7d03b23c5b1f806b68ee3ad8637_MD5.jpeg]] can repeat forever
	- soln: another consensus alg that might not terminate!?!? (leader election)
- MultiPaxos: to decide sequence order or operations
- use for leader election, configuration consistency, etc.
- ex: libpaxos in forge
- [Luis Quesada Toress lecture on Paxos](https://www.youtube.com/watch?v=d7nAGI_NZPk)
- 
	- 