---
Date Created: 8/26/2024 12:28 PM
Status: "#adult"
Description: data prep code cheat-sheet, data-type/outlier/missing transformations
---
Tags: [[AI]] [[ML]] [[Python]] [[Pandas]] [[Matplotlib]] [[Seaborn]] [[NumPy]]
## 1\. define **unit of analysis** (row): problem statement (fraud: transaction units/examples)

## 2\. sampling data (i.e. random):
- dataset size? diminishing returns at larger datasets (**empirical evals)**  
- different models for subsets? (ie. **web-type** transaction vs **physical**?)  
  - if not divide population but should: bad at generalizing either. 
  - ![[d22798bc251fae20d35b83dc9c2486f4_MD5.png#invert]]
  - **IID samples** (independent but identically distrib.) [[statistics]]
	  - in the same subset/subpopulation, best type) i.e. random 30%  
- **underrepresented** → less accurate models. balance **features** (race) vs **labels** (crime)  
  - solution: resampling to equally represent all data:  
  - `np.sum(df['sex']=='Female')`, or `df['sex'].value_counts()['Female']`
  - `df['sex'].value_counts()` → returns dict ordered by freq  
  - `df.groupby(['sex', 'income']).size()`:
	  - **![[cc04b2d6dc2bbc00fb624f679c565342_MD5.png#invert]]→ resample →  ![[4f9f74e1b47fbae58a0d59ec73dff6ef_MD5.png#invert]]
  - **resample** random copies from underrep group: 

| Step                                | Code                                                                                 |
| ----------------------------------- | ------------------------------------------------------------------------------------ |
| Filter underrep group from OG data: | `df_target = df[condition]`                                                          |
| Sample it                           | `indices = np.random.choice(df_target.index, size=additional_needed, replace=False)` |
| Append sample to OG sample          | `rows = df.loc[indices]`<br>`df_balanced_subset = df.append(rows)`                   |
- get random 30% of dataset:  
	- **numpy:** `df = df.loc[np.random.choice(df.index, size=int(0.3 * num_rows), replace=False)]`
	- **pandas**: `df = df.sample(n=int(0.3 * num_rows), replace=False, random_state=1)
## 3\. feature engineering (\!\!affects problem\!\!)

### define a label. 
- when is this hard?  
	- multiple relevant labels, subjective, abstract
	- don't directly observe label (i.e. user satisfaction, recommendation feed)  
- manual vs deep [[neural networks]] (image recog, nlp)  
	1. think like scientist: likely causal factors, test emprically  
	2. seek input of experts
	3. prioritize and iterate (save time on prepping features, stop if dim. returns)  
### A) map concepts to data (data src → data table) [[evaluation]]
- select, filter, fetch from data src  (i.e. POS-tagging for geographic data mentions)
- define feature and label
- obtain data src (i.e. machine logs, databases, status code map --> english, docs, api)  
### B) transform data types for modeling [[data preparation]]
- types of feature transformations:  
	![[3537eb30af1850972009a2a89c667f4a_MD5.png#invert]]
	![[a352c65a91f050fcc5e7b5c358ca08ac_MD5.png#invert]]
- most are simplification of data
	-  ordinal (1,2,3,4,5)     →     high/low(\<3, \>=3)  
	- time-to-event     →     binary classification (Time\<k)  
- but **one hot encoding**: *necessary* for categorical cols: ![[efd1d40b57b203393b6f78730c5a2202_MD5.png#invert]]
- **interaction term**:  multiply two cols sometimes stronger feature than each individually  
- **binning**: ordinal nature of numeric feature not as important, convert to histogram  
- **scaling**:  standardization (mean=0, sd=1) vs minmax scaler: (i.e. min=0, max=1)  [[statistics]]
## Data Types:  
**![[7a00204938e45017b99a13f0d5675987_MD5.png#invert]]**
- **numeric:** regression (continuous) vs specialized for integer  
- **categorical**: not inherently ready for ML!!! 
	- **ordinal**: ordered categorical
		- i.e. 2 star vs 4 star rating, **you like 4 more but not twice as much\!**
		- convert to **binary indicator features**  
		- regression (but i.e. \-3 may not make sense) or classification  
	- **nominative:** 
		- encode distinct categories (ie strings) as numbers → NOT NUMERIC → **classification**  
### Code Cheatsheet

#### Data-type Code
- **dtypes** guide:    **![[b6dbbc53a546bede10c5f222436cdf1f_MD5.png#invert]]
  - `df.dtypes`
	 **![[a5435d568214b39e218bf091270eb9ff_MD5.png#invert]]** 
	- of specific column: `pd.api.types.infer_dtype(df[col])`  
    - `is_string = (df.dtypes == 'O')  `
    - `df[df.columns[is_string]].nunique()` grabs string cols and prints num unique strs
    - `df.columns[df.dtypes == 'object'].tolist()` filters only object type cols
  - `df.describe(include='all')`
	**![[0be12c4147c8ae32f1c3174f4639871c_MD5.png#invert]]**  
- see object columns: `list(df.select_dtypes(include=['object']).columns)`
- \# uniques for multiple columns: `df[col].nunique() `
	 **![[8cc52614ec2ae1709db1a449b20cd2a5_MD5.png#invert]]** 
- **create one-hot encodings** for ServiceArea 
	with skilearn:
```
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse=False)  # sparse=False ensures dense output
df_enc = enc.fit_transform(df[to_encode]) #fit encoder to the list of cols
df_enc = pd.DataFrame(df_enc, columns=enc.get_feature_names(to_encode))
df_enc.columns = enc.get_feature_names(to_encode) # match numerical colname

df.drop(to_encode, axis=1, inplace=True)
df = df.join(df_enc)
```
	with NumPy:
```
	for value in top_10_SA:
		df['ServiceArea_'+ value] = np.where(df['ServiceArea']value,1,0)
	df.drop(columns = 'ServiceArea', inplace=True)
```
	with Pandas:
```
df_encoded = pd.get_dummies(df['Married'], prefix='Married_')
df = df.join(df_encoded)
df.drop(columns = 'Married', inplace=True)
```

| Code                                                                                                    | Description                                                          |
| ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `df['col'] = df['col'].astype(int)`                                                                     | convert col of strings to ints                                       |
| `df['price'] = df['price'].str.replace('$', '')`                                                        | replace in col of strings                                            |
| `df['size'] = pd.Categorical(df['size'], ordered=True, categories=['S', 'M', 'L', 'XL']`)               | creating an ordered categorical var                                  |
| `cond = (df['col']=='Cat-A')`\|`(df['col']=='Cat-B')`<br>`df['col'] = np.where(cond, 'Cat', df['col'])` | consolidate Self-emp-inc, Self-emp-not-inc values into just self-emp |
| `df['col'] = pd.Categorical(df['col'], ordered=True, categories=['S', 'M', 'L', 'XL']`)                 | convert strings ('O') to ordinal data ('CategoricalDtype')           |
| `df['col'].fillna(value=mean, inplace=True)`                                                            | replaces # missing values with mean                                  |
| ` df['col'].value_counts().head(10).index.tolist()`                                                     | top 10 most freq. val of col                                         |
|                                                                                                         |                                                                      |

#### General Code

| Code                                     | Description                  |
| ---------------------------------------- | ---------------------------- |
| `df['workclass'].unique()`               |                              |
| `df = pd.read_csv(filename, header=0)`   | read csv as df               |
| `df[name].value_counts().head(10).index` | get column names of a Series |
| `df.loc[condition == True]`              | grab only condition rows     |
| condition = condition1 & condition2      | combine conditions           |
## 4. Exploratory Data Analysis in data prep
- Purpose? 
	- helps understand structure, distrib/outliers. and patterns/relations within data
	- find any potential issues that may require further investigation or preprocessing
	- help communicate findings effectively for decision-making
- distribution? ([[Cheat Sheet Matplotlib, Pandas and Seaborn.pdf]])
	- look for redundancy, correlation   

#### Linux

| Code                                            | Description                                                   |
| ----------------------------------------------- | ------------------------------------------------------------- |
| ` cat data.txt` \| `wc -l`                      | gives \# lines in file                                        |
| `head -1 data.csv` \| `tr ',' '\\n' `\|` wc -l` | converts delimiter with newline then count \# lines (\# cols) |
| `head -5 data.csv`                              | prints first 5 lines of file                                  |

#### Python				   

| Code                                                                  | Description                           |
| --------------------------------------------------------------------- | ------------------------------------- |
| `df.describe().loc['25%']['age']`                                     | get 25th percentile age               |
| `df.describe().loc['max'] - df.describe().loc['min']).idxmax(axis=1)` | get feature w/ greatest range         |
| `df.describe().loc['std'].idxmax(axis=1)`                             | get feature w/ most variation         |
| `df[][].mean()` <br>`np.mean(df[col])	`                               | get mean                              |
| `np.any(df_summ.loc['min'] < 0)`                                      | any negatives?                        |
| `df[df['native-country'].isnull()]`                                   | grab nulls ('NaN', 'nan', None, etc.) |
| `df.drop(['col1', 'col2'], axis=0, inplace=True)`                     | delete col1 and col2                  |
#### Example insights from Plots:  
- for quantitative labels:
	- **skew**: mean != median. can worsen models.  
	- label **imbalance** if mean << or >> middle of range
- **negative** labels:  does it make sense? encode error?  
- feature selection, model insight, correlation/redundancy etc  
- **sub-clusters**: two peaks? maybe separate clusters/models before train/predict?

[[Matplotlib]]
##### Univariate Plots (distrib.):
  - heavy skews! **![[289df1cd24083e018f3e68523b6dfef7_MD5.png#invert]]
	- large range heavy skew to left, so **log_e** --> bell curve  
	- `sns.histplot(data=df, x="age", log_scale=True)`		  
	- `plt.ylim(0, 600)`
  - categorical works too (code below fixes overlap**![[06ef3e90277b655bc774383a1927e775_MD5.png#invert]]**)  
    - first convert col to ordinal (will also order histogram) 
	- `fig1 = plt.figure(figsize=(13,7)) # resize plot for readability
	- `sns.histplot(data=df, x='edu', hue='income', multiple='stack') # color-code`
	- `plt.xticks(rotation=45) # rotate x-axis ticks labels  `
    - **![[c05a45713159b7098bc46c810e737be8_MD5.png#invert]]
    - **error bars: 95%** confidence interval for plotting the average: using `plt.errorbar()`
##### Bivariate plots (corr.)  [[statistics]]
  - **redundancy:** if 2 features highly correlated, drop one (feature engineering)
  - **pearson's correlation visualizations:** (only by *linearity* tho)  
    - !**![[035ad5e3c14b322b533612649615a14c_MD5.png#invert]]**![[f5c207d685322c311b46edca6f037add_MD5.png#invert]]`sns.scatterplot(data=df) ` 
To plot bivariate (just 2 cols):  
- new df:      `df2 = pd.DataFrame({'hours': df['hours'], 'years': df['years']})  `
- `sns.pairplot(data=df_sub, hue = 'income', plot_kws={'s':3}, corner=False)`
	- **color-coded** by label, 'income'
	- **plot_kws** dict of options, i.e. {'marker':'+'} changes symbol
	- **corner=** True to omit redundant graphs (symmetric upper & lower triangles)
	- ![[59108d0c6dd12e91858743483c2301a5_MD5.png#invert]]
	- **Analysis:**
		- diagonals: univariate (i.e. age distrib.), rest correlation
		- trend 1: for top right plot (hours-per-week x age), more clumps bottom and top left = young/old less hours per week
		- trend 2: for bottom middle plot (capital-gain x hours-per-week) vertical line of orange on far right so unusually high capital gain correlated with >50k income. 
- `sns.pairplot(df_corrs, kind='kde', corner=True)`
	- **Kernel Density Estimator:**   visualizes joint prob density of two cont. vars. higher mountain/contour = higher density
	- ![[1e59ea2d9c1384b991aab2abefc8d556_MD5.png#invert]]
	* **Analysis:**
		* bold vertical ridge around x=40 in bottom middle plot = hours-per-week most commonly 40 for all edu lvl (typical 9-5 job, 8hrs/day x 5 weekdays = 40)
		* bottom middle plot lower half not spread over towards 100 hrs/week = less typical for lower edu years to work over 50 hours/week. 
###### **Correlation**  
- **covariance**: how much two vars vary from their means![[a46cb2d30a66ec38875c6dce158eb76d_MD5.png#invert]]
- **correlation** = standardized covariance to [-1,1] by dividing w/ std dev. ![[3a1969ef6e1443edc43cbdfe951e8ca4_MD5.png#invert]]to easily compare btwn features
	- corr = -1 perfect negative linear
- top two correlated col names: `list(corrs.sort_values(ascending=False).index)[:2]` 
- **correlation matrix**:  `corr_matrix = round(df.corr(),5)`
	- diagonals = 1 (how correlated to itself)
	- symmetrical with respect to the diagonal.
# 5. Data cleaning [[data preparation]]
- encode categorical variables
- scale numerical features
## Outliers
- (<= 1% freq) outliers: univariate or/and bivariate in nature, so define some cutoff
- **![[d8883ce13dcde5d8e00f2dd28bf93470_MD5.png#invert]]**
  - **z-score** ![[2d6169f2c8f301f111b09b354864ebb3_MD5.jpeg#invertW]] if +/-3 (exceeds 3 std devs from mean), outlier.
    - for specific col: `stats.zscore(df['col'])  `
    - for all numeric cols in df: `df.select_dtypes(include['number']).apply(stats.zscore)`
  - **interquartile range** (IQR)
	  - thresholds:  (Q1 \- k\*IQR,  Q3 \+ k\*IQR)	k times greater than IQR. 			  
	  - IQR = dist btwn 25% and 75% percentile values	
	  - `IQR = np.percentile(df['col'], 75) − np.percentile(df['col'], 25)  `**![[c3c7881a3e3bfc8d94fdf145b575a9fd_MD5.png#invert]]**
  - **Dealing w/ Outliers:**  
    - just drop them (rare or error-prone)  
    - winsorize: cutoff 1% (replace <1% and >99% with 1% and 99% val)**![[9163ebd8f74da034703491bdd79fbfeb_MD5.png#invert]]**
      - `df['col-win'] = stats.mstats.winsorize(df['col'], limits=[.01, .01])`
## Missing values  
- `df.isnull().sum()`
- `df.isnull().values.any() ` 
- `df.isna().sum() > 0`:  **![[fd44ed416566e46a2027918704fbbb82_MD5.png#invert]]**
- Option 1: drop the example/column  
	- `df.drop(['Married'], axis=1, inplace=True)`
	- `df['ServiceArea'].fillna('unavailable', inplace=True)`
- Option 2: imputation:  replace with mean/median  
- Option 3: interpolation:  predict using other features (aka create model with missing feature as label for every missing example \$\$\$)