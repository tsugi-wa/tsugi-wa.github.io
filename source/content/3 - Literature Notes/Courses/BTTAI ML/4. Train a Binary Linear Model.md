---
Date Created: 9/4/2024 12:02 AM
Status: "#baby"
Description:
---
Tags: [[supervised]]

**Logistic Regression:** 
- supervised binary classification + implicit confidence (probability)
- assumes linear relation btwn features and label (linear feature-weights computations)
- ![[Pasted image 20240904002156.png#invert|300]]
	- non-linear log-loss ex: [[neural networks]]
	- [[modeling]]: deciding between linear and nonlinear: 
		- linear pros: small data, speed, ease
- predict the probability of an outcome
# Process
- model is essentially one bias (scalar) and a weight for each feature. 
- ![[Pasted image 20240904011739.png#invert]]
- ![[Pasted image 20240904012116.png#invert]]
## 1. linear step:
- $z(X) = \alpha + w_1x_1 + w_2x_2 + \dotsb + w_nx_n$
### NumPy Operations Cheatsheet
- dot product dimension rules: inner dimensions same (i.e. 1st col == 2nd row)

| **Operation**                                                                                                        | **Explanation**                                                                                      |
| -------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| `X[:,0]`                                                                                                             | get first column                                                                                     |
| `np.reshape(np.arange(n*n), (n,n))`                                                                                  | if n=4,  outputs\[\[0,1,2,3],\[4,5,6,7],\[8,9,10,11],\[12,13,14,15]]                                 |
| `X+np.arrange(3)`                                                                                                    | adds vector \[0,1,2] to each row. \[\[1,1,1],\[2,2,2]] -> \[\[1,2,3],\[2,3,4]]                       |
| `X+np.arrange(n).reshape(n,1)`                                                                                       | adds vector \[0,1] to each col. \[[1,1,1],[2,2,2]\] -> \[\[1,1,1], \[3,3,3]]                         |
| `np.linalg.inv(X)`                                                                                                   | inverse matrix:  $X \cdot X^{-1} = I$                                                                |
| `np.reshape(np.random.random(3*3), (3,3))`                                                                           | generate radom 3x3 matrix                                                                            |
| `X.T`                                                                                                                | get the transpose of the matrix                                                                      |
| `np.ravel()`                                                                                                         | turns array into 1D array                                                                            |
| `np.cov(X, rowvar=False)`<br>                                                                                        | compute covariance matrix<br>rowvar=False treats col as features not row<br>or manually (X_centered) |
| `X_means = np.mean(X, axis=0) # per col`<br>`X_c = X - X_means # X centered`<br>`cov = np.dot(X_c.T, X_c) / (N - 1)` | compute covariance matrix MANUALLY<br>$({X_c^T \cdot X_c})/({N-1})$                                  |
| `np.round_(X, decimals=10)`                                                                                          | rounds by 10 decimal places                                                                          |
|                                                                                                                      |                                                                                                      |
## 2. inverse logit:
- uses sigmoid/inverse logit to convert XW to a probability/prediction: $\sigma(z) = P(y|X)=(1 + e^{-(WX+b))^{-1}})$
- ![[Pasted image 20240904012546.png#invert|300]] 
- note may need to transpose ``X.dot(W.T)
```
def log_reg_predict(X, W, b): # same as model.predict_proba()
	# X and weights are array, bias is float
	XWb = X.dot(W.T.ravel()) + b # or (X*W).sum() + b
	P = 1 / (1+np.exp(-1*XWb)) # Prob(Y|X)
```
## 3. mapping 
- map prob to predictionlabel using threshold (i.e. >0.75 is spam): 
- $y = \begin{cases} 1 & \text{if }P(y| X)>.75\\ 0 & \text{else}\\ \end{cases}$



# Loss Functions:
For pure evaluation vs for numerical optimization, NOT same types
## Log Loss (binary cross-entropy)
- for binary regression
- $L=-\frac{1}{N}\sum_{i=1}^{N} (y_i log(\hat{y_i})+(1-y_i) log(1-\hat{y_i}))$
```
def log_loss(pred, y):
	return -1*(truth * np.log(pred) + (1-truth) * np.log(1-pred))

def log_cost(pred, truth):
	pred = np.clip(pred, 1e-15, 1-1e-15) #avoid /0 or log(0)
	return -1*(truth * np.log(pred) + (1-truth) * np.log(1-pred)).mean()
```
![[Pasted image 20240904001456.png#invert]]
quantify varying degrees of correctness:  exponential worse loss

## MSE (mean squared error), 
- for general logistic regression
- $L=\frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y_i})$
```
def mean_squared_error(pred, truth):
	return (pred - truth)**2

def mean_squared_error_cost(pred, truth):
	return (pred - truth)**2.mean()
```
![[Pasted image 20240904002020.png#invert|300]]
## Zero-one loss
- ONLY to evaluate multiclass/binary classifiers
- used in accuracy evaluation metric
- $\mathbb{I}(\hat{y_i}\neq y_i)=\begin{cases}1  & \text{if }\hat{y_i}\neq y_i\\0 & \hfill\text{if }\hat{y_i}=y_i\end{cases}$
- simply counts # erroneous predictions.
- cost func: $L=\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(\hat{y_i}\neq y_i)$

# Hyperparameters
##  C: complexity
less regularization when C high

# CODE
```
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss

X_train, X_test, y_train, y_test = train_test_split(X,y)

model = LogisticRegression(C=1)
model.fit(X_train, y_train)
probs = model.predict_proba(X_test)) # logreg already returns probabilities
```

optimization algorithm
gradient descent

