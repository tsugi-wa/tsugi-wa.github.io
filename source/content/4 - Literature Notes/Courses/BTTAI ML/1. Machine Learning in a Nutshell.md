---
Date Created: 8/26/2024 10:13 AM
Status: "#adult"
Description: general python ML library cheat-sheet, top-down view of ML workflow
---
Tags: [[AI & ML]] [[AI & ML]] [[Python]]

![[AI & ML#ML Pipeline]]

## **ML model**: 
**output**: data representation of observed patterns. used for prediction/decision-making function
instead of instruct actions, use model algorithms to instruct it how to learn
- generalize to unseen  
- behavior thru experience (spam filter) or on the fly (adapting user feed)
- **[[deep learning]]**: neural networks, used in LMs, speech, vision  
- **[[statistics]]** (i.e. linear and logistic regression)  
	- deferential: degree of variation/distribution
	- inferential: generalize given subset  
- **[[data science]]**: find patterns, ML \= automated data science
**[[feature vector]]: grouped together in vector, d-dimensional (i.e. \<height, weight, sex, ‚Ä¶age\>**
### [[supervised]] learning  
- multiple iterations of training, evaluation, and optimization to improve  
- **regression**: quantitative  
- **classification**: qualitative (categorical)
	- no order/association between classes (i.e. colors OK vs rounded height class BAD)  
	- good for image classification (i.e. cat,dog)
- **training phase**: run standard alg on labeled training data, which finds best correlated features 
- **prediction phase**: inference/testing to make predictions  
- **overfitting**: not to perfect at training data, but generalize to unseen  
- **cold start**: don't have the right data yet. need experiments 
### [[unsupervised]] learning:
- find clusters/segments of similar ppl to group together similar musical taste listeners  
- can be first step, guide toward later supervised learning  
- doesn't know what clusters mean, so we can manually label afterwards  
	- add subjective interpretation: stylized/descriptive based on other info?  
**![[34476970007295edd22b3e5aa53cf146_MD5.png#invert]]**
### Practical ML Considerations
- need right data and ethical data  
- **just a tool**, application use most important  
- label (physical therapist rating) \+ features (joint angles)  
- heuristic (angle threshold)  
- thrive bc (1) alg  (2) data  (3) gpu  
- ethical risks: large scale automated, marginalized groups  
- system risks: more complex systems more failure points
- **Data lake**: with both structured, processed data and unprocessed. SQL

## ML Lifecycle: Cross-Industry Standard Process for Data Mining (CRISP-DM)
**![[c1bfa9f8bc61305b55247bf6a94b86dc_MD5.png#invert]]**
- **Business understanding:** problem formulation? written project brief (constraints, timeline, goal) 
	- why need ML?  What if non-ML?
	- what data needed?  
	- potential risks? 
	- constraints? costs?
		- iterative/heuristic until incremental improvements not worth  
- **Data understanding/preparation**: right data?  longest stage  
- **modeling/evaluation**: shortest stage (independent techniques)  
- **end-to-end deployment**: full stack, usage, software dev (MLE vs data scientists)   [[applied AI]]
	- pure knowledge  
	- process & theory  
	- subjective evaluation  
	- subjective tradeoffs

### Example: filtering algorithm cluster YouTube feed
- recommend similar items to what you viewed, see same groups  
- scoring every item with supervised COSTLY!
	- so use **hybrid approach**: first unsupervised then \+ supervised for each cluster subset  
- always multiple ways to solve, so make right choices\!

## Python Libraries

`ls \-ltr \*cell\* ` looks for word "cell" in file  
`jupyter notebook`
```
import os
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
```

**[[Scikit-learn]]**: most important ML algs and evals/cross-validation  
**[[SciPy]]**: computation math, improve, proven algs  
**[[Pandas]]**: Data manipulation and analysis, structures, functions
- **Use:** For cleaning, transforming, and summarizing data
**[[NumPy]]**: large, multi-D arrays, array functions
- **Use:** For numerical computations, i.e. linear algebra, stats, storing images
**[[Matplotlib]]**: static, interactive, and animated visualizations
- line plots, scatter plots, histograms, etc. 
- **Use:** creating publication-quality visualizations and exploring data distrib/relations
**[[Seaborn]]**: simpler than Matplotlib 
- themes, color palettes, and functions (i.e. scatter plots, pair plots, and heatmaps)
- **Use:** exploratory data analysis and visualization of *statistical* relationships

**print infinite rows:** 
```
pd.set_option('display.max_rows', None)
pd.reset_option('display.max_rows')
```
## IPython & Linux
[[Cheat Sheet Linux and IPython.pdf]]

[IPython](https://ipython.readthedocs.io/en/stable/index.html): interactive Python (data load/analysis/plots): , used in Jupyter notebooks
- [magic functions](https://ipython.readthedocs.io/en/stable/interactive/magics.html) like `%ls`, `%run src.py`,` %timeit`
	- `%lsmagic` see all magic functions,  `%magic_func_name?` to get info
- **help:**
	- `help(obj_name)`, `help(data_type)`, `obj/func?`
	- `dir(obj_name/data type)` prints all methods/attributes 
- `%history` see input cmd history
- `%pdoc obj_type` to see docstring
- `%who` describe what variables in main namespace, `%whos` includes summary
- linux-like: `cat`, `cd`, `cp`, `less` which views a txt file, `ls`, `man`, `mv`, `pwd`

## [[Cheat Sheet ML Python Packages.pdf]]

#### Seaborn

| Code                                      | Explanation                                                                                                |
| ----------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `sns.histplot(x=df.col_name)  `           | plot univariate distrib. of col in df                                                                      |
| `sns.pairplot(data=df, hue='income')`<br> | plots grid of pairwise correlation plots and univariate distrib. in diagonal. Color by income category/val |
| `plt.savefig(path_to_save)`               | saves graph as a local image                                                                               |
| `plt.show()`                              | shows graph                                                                                                |
### [Pandas documentation](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
![[Pasted image 20240826121224.png#invert]]

| Code                                                                                                         | Explanation                                                            |
| ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| `df.describe()`                                                                                              | see summary of **numeric data cols** (shape, common val, freq, etc)    |
| `df.describe(include='all')`                                                                                 | to include summary of **non-numeric** cols                             |
| `list(df.columns)`                                                                                           | print all column headers                                               |
| `df['col'].unique()`                                                                                         | list all unique vals in column                                         |
| `df.nunique()`                                                                                               | list number of unique vals in each col                                 |
| `my_list = [['GC', 'Grace'], ['AK', 'Anya']]`<br>`df = pd.DataFrame(my_list, columns = ['initial', 'name'])` | creating df from lists                                                 |
| `my_dict = {'initial': ['GC','AK'], 'name': ['Grace', 'Anya']}`<br>df = pd.DataFrame(my_dict)                | creating df from dicts                                                 |
| `df['grade'] = ['A', 'B']`                                                                                   | add column of matching length                                          |
| `df = df.rename(columns={'old_name':'new_name'})`                                                            | rename a col                                                           |
| `df = df.sort_values(['grade'])`                                                                             | sort df by grade                                                       |
| `df = pd.concat([df1,df2]).reset_index()`                                                                    | concat two dfs by col(1) or row(0), NaN if missing cols.               |
| `df = pd.merge(df1, df2, on='id', how='inner')`                                                              | join two dfs by 'id' column, only include rows where 'id' val in both  |
| `df = pd.merge(df1, df2, on='id', how='outer')`                                                              | include all rows of both.                                              |
| `df = pd.merge(df1, df2, on='id', how='left')`                                                               | all rows of df1 and matches in df2                                     |
| `bool_df = df['age'] > 30`<br>`only_trues = df[bool_df]`                                                     | bool_df is Series of True or False, only_trues filters out False rows. |
| `col-name = 'department'`<br>`df.groupby('col-name')['number-col'].sum()`                                    | ![[Pasted image 20240826120958.png#invert]]                                   |

### Numpy: 
[[Cheat Sheet NumPy Array.pdf]] 

| Code                                    | Explanation/Output                          |
| --------------------------------------- | ------------------------------------------- |
| `a = np.arange(6).reshape((2, 3))`      | `[0,1,2,3,4,5]` ‚Üí `[[0,1,2], [3,4,5]]`      |
| `a = df.values.flatten().tolist()`      | `pd.DataFrame([[1,2], [3,4]]) ‚Üí  [1,2,3,4]` |
| `7 * np.ones(4, dtype=int)  `           | `[7,7,7,7]`                                 |
| `matrix = np.array([[1,2,3], [4,5,6]])` | `[[1,2,3], [4,5,6]]`                        |
| `k = 0`<br>`np.triu(arr, k=k)`          | ![[Pasted image 20240826122218.png#invert]]        |
| `k = -1`<br>`np.triu(arr, k=k)`         | ![[Pasted image 20240826122346.png#invert]]        |
| `np.random.rand(1, 20)`                 | generate 1x20 array of random nums          |
| `np.any`                                | checks bool array for any Trues             |

# Unit 1 Questions
## Part One: Using ML for Industrial Decision Making

In this part of the assignment, you will identify a real-life company
and a product, feature, or application that is driven by a supervised
machine learning method. Answer the following questions based on that
real-life example.
### 1.  Your chosen ML example?
This example is a UCLA-run AI research project that I just joined for their bionics lab. We are training a deep learning model to automate physical therapy scoring of upper limb mobility for stroke patients.
### 2.  Business objective of ML algorithm?
The project\'s ultimate aim is to create a virtual reality game that can provide a more fun, detailed, and frequent monitoring of stroke patients over the course of their rehabilitation and long term. Automated scoring of their mobility can be performed at home and far more frequently than through visits to a physical therapist. Also, the goal is to take advantage of the quantitative measuring of angular movement and range between the shoulder, elbow, and finger joints through sensors to create a higher resolution scoring system than the eye-balled categorical 0,1, and 2 scoring that physical therapists give. This can motivate patients through showing them the gradual improvements they\'ve made over time and provide a more insightful profile of their mobility.
### 3.  Label? 3 Used features?
As of now, we are focused on 0-1-2 qualitative scoring. The majority of our example data points are from healthy subjects wearing our sensors and mimicking various degrees of mobility without a physical therapist, so our labels are from a research paper's heuristic (a   simple threshold of each joint\'s angle for 0, 1, or 2 scores) instead. The rest of our data points are from real stroke patient data with a physical therapist's scoring as the label.  
For features, we use temporal angular data (extracted from both the sensors and five computer vision cameras). For the hand specifically (what I am working on), we have the angular data of the index finger and thumb in relation to the palm for the x, y, and z axis which amounts to a total of six features all over time. We use these features to determine the mobility scoring of the patients for each of the seven hand tests.
### 4.  Explain why ML right tool to achieve objective. 
(To help your thought process, think about what the alternative, non-ML solution could have been. Note also that sometimes it may be the case that the use of ML by the company is not well motivated.)

One non-ML alternative is the traditional approach to Fuger-Meyer assessments in hospitals and physical therapy centers - have either a patient visit the center once in a while to get scored by a physical therapist or have an in-patient with more frequent monitoring and scoring by a physical therapist. In my opinion, equipping stroke patients with sensors seems to be far more trouble in terms of price, accessibility, and lack of a mass-produced market for these sensors (our sensors are 3D printed!) when compared to this alternative of visiting a physical therapist. I think this project is better suited as a prelude to a future where this is far more probable and affordable than as a practical solution right now, but I do see its potential advantages over a physical therapist (see my answer to question 2).  
Another alternative is to use the sensors but instead of ML, apply only the heuristics of the research paper "[Automated Evaluation of Upper-Limb Motor Function Impairment Using Fugl-Meyer Assessment](https://ieeexplore.ieee.org/document/8048514)" onto the sensor's angular data to determine the patient's score. This means using a linear threshold (i.e. the dominant joint angle of a test surpasses some threshold ùõå then the score for that test is 2) instead of a ML model trained on labeled angular data. I also had trouble understanding why a ML model was necessary in the first place if it was far easier to just use this existing threshold. Especially since my project lead told me that the existing research paper's threshold has a higher accuracy (92%) than our current ML model combining LSTM and RNN that requires lots of tedious data collection (a visit to measure one stroke patient aka data point took about 6 continuous hours of setup and other busywork). I asked this exact question to her and she said that the ML approach can combine data from the Fugel-Meyer test with many other tests that were also performed in our data collection which is not used in the heuristic study, for the ultimate goal of creating a reasonable (provably useful) quantitative scoring (i.e. 1.3 vs just 0-1-2) system that is not available in either the physical therapist or heuristic scoring. The benefits of this can also be seen in my answer to question 2.**

## Part Two: Recognizing ML Problem Types

In this part of the assignment, you will take your example from the previous part and will further analyze its problem type, classification or regression.
### 1.  What problem type? Explain why classification or regression
At first we are trying to replicate the scoring system of the existing physical therapy practices for Fugel-Meyer assessments, which is the categorical label of 0, 1, or 2. We are using heuristics and real physical therapist scores as the labels to our training data so we are doing supervised learning and classification.

Then, we are trying to improve on this system by offering a higher resolution labeling of the patient's mobility with things like 0.1, 1.4, etc. by comparing it to several other features measured from tests alongside the Fugel-Meyer assessments. So this is also supervised learning but regression. However the label part is tricky because we only technically have the labels for 0,1,2 so we'll be tweaking our regression models via the quantitative data across multiple tests (aka how reasonable the qualitative scoring is).
### 2. Give another example of a classification or regression problem that you interact with in your daily life, or one that companies or governments might use.
 When you make a listing to sell something, you first type in the title and then the description of the item or service and photos. Then when you go to the next page in the listing creation process, there are options to enter the brand, category, subcategory, model, etc. of the item. The company (i.e. ebay, offerup, etc.) streamlines this process for sellers by automatically choosing the category of your listing that they think best fits your item. For example, if I type "small skateboard" as the title of my listing, the company guesses that the category is "sports & outdoors". Since the company has only a finite list of distinct categories that do not relate to each other, the company is using classification to make it easier for sellers to post things and thus easier for buyers to find what they want.

## Part Three: The ML Lifecycle 

Imagine you work for a telecom company. The management wants to address the problem of customer churn\*. Your task: predict which customers likely to churn. Describe the steps that you would take to address this problem. 
-   Why useful to predict future customer churn? How serve business objectives?
-   How would you further formalize the problem? inputs, target label
-   What methods (supervised or unsupervised) would be appropriate to use? Why?
-   What kind of data ideal, and what kind of data expected to be available?
*Customer churn is the loss of customers or clients and happens when customers decide to stop doing business with a company.*

### **Business Understanding**
By predicting which customers are more likely to churn, the telecom company can target these customers with incentives to continue doing business with them such as exclusive deals and freebies which would be detrimental to the company if these were offered to everyone. This can maximize the benefits of offering these deals while minimizing the cost. Or they can find similarities in those particular customer bases such as these customers being of a certain age/income bracket or located in an area that is suffering from unusually high maintenance and interference issues when using services provided by the telecom company because of variations in infrastructure. If they can tie these similarities to probable reasons for these customers choosing to stop doing business with the company, the company can be more equipped to mitigate these reasons to reduce customer churn, by finding the places that need the most infrastructure repair or building, for instance. Again, this can potentially increase the effectiveness of any customer churn mitigation efforts by specifically targeting areas or customer bases that are most likely to stop doing business and also provide a more detailed and precise weighing of customer features than something like general heuristics.¬†

### **Data understanding/preparation**
The best kind of data for this problem would be to have as much data as possible about the customer that might be relevant to their opinion of the telecom company, such as income bracket, exact location of their residence, how often they travel and how often they use services from the telecom company while traveling, age, gender, occupation, household size/company affiliation. Then it would be compared to their interactions with the telecom company itself, which is the kind of data that is far more likely to be at the disposal of the telecom company, such as a history of their payment plans (i.e. cost, date, type of plan, duration, etc.) and logs of their contact with customer service via phone, chat, sms, etc. (i.e. message history, category of question, the area of customer service that was contacted, date and frequency, their satisfaction rating in the survey afterwards). Then the label would probably be implicit in the history of their payment plans (canceled a plan, or didn't renew an expired plan for x months or something) which can be extracted as the label of the training data. Thus all these numerous features must be normalized somehow to easily compare with each other and the label extracted from the payment logs in the lengthy process of data cleaning and preparation. For instance, the payment logs can provide features like the total duration of all their plans with the company, number of plans in each of their categories (monthly, yearly, etc.), etc. Their message history can provide features such as a raw count of distinct tickets raised in each category of issue, their average satisfaction scoring afterwards (if available), a bag of words or other NLP approach for analyzing the actual message content of their customer service contacts, etc.¬†

### **Modeling/evaluation**
The given problem says to "predict which customers are likely to churn" so I will interpret this as a binary classification problem, which, given the data point of a specific customer in the format of the feature vector, predict whether the customer will likely churn "yes" or likely stay with the company "no". As I said above, the training data can have its labels (churned or not?) extracted from the payment log or some other method so the model to be used for this problem will be supervised learning. This is because supervised learning can provide a more detailed association between customer features and the observed outcome (churn) than unsupervised learning, which provides classification for data points that are less obvious or clean-cut in their categorization. Then, for the actual model training, the inputs would be the normalized features of all the customers available from step 2 in the form of a fixed feature vector plus the extracted label of churn or not. And for the desired model, the input would be the features without labels and the output would be the labels. To evaluate the effectiveness of the model, we need to split the existing labeled data into training and testing data, train the model only on the training data, and then compare various evaluation scores on both test and train data and iteratively tweak and improve on the model until the scores are high on both seen and unseen data.¬†
  
### **Deployment**
Finally, we will have to figure out how to use this prediction model to target the audience that needs the most churn mitigation actions against. This will vary depending on the company's policies and a little out of the scope of my task which was just to predict the customers that will likely churn. I have brainstormed some possible use cases in the beginning of this response under "Business Understanding".**