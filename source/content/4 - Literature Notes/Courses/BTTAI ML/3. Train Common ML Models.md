---
Date Created: 9/1/2024 4:15 AM
Status: "#adult"
Description: understanding KNN and Decision Trees
---
Tags: [[Scikit-learn]] [[modeling]] [[evaluation]] [[supervised]] [[evaluation]]
- K-nearest-neighbor vs decision trees: **non-parametric** (so generic vs linear which assumes linear)  
- minimize expected vs training loss. avoid overfit.   
- **split** into train validation
	- ![[6bd3d8fc00c513d8efd7b150a1c33513_MD5.png#invert]] 
	- evaluate on validation to adjust hyperparams
	- and when finally ready, test at end
- **hyperparams:**  
	- Used to adapt a model to a particular setting  
	- Specified by the practitioner w/ heuristics and tuned  
	- ![[Pasted image 20240901031537.png#invert]]
- scikit-learn common api across diff models, easy to try them on same data
## KNN 
- [[Cheat Sheet K-Nearest Neighbors.pdf]]
- for regression (continuous) or classification (categorical)
- prediction: store and query existing data (no explicit "model", is **instance-based**)  
- boundary on query (circle around by distance) and get vote / average of neighbors
- clearer clusters in data â†’ better predictions with KNN  
- ex: look at k (i.e., 3\) closest neighbors of query: **![[fcae3063442a974181f30dc054ca4c68_MD5.png#invert]]**
### **Distance function/metric**
- **![[2ec16f5aa2d061fcc3591aef6e4b9dd2_MD5.png#invert]]**
- Euclidean maybe less optimal for normalized/other structures 
	- to compute Euclidean: `np.linalg.norm(array2-array1)`
* k=1 decision boundary:  
  - **![[55e001522f61921854c406f09e0398f5_MD5.png#invert]]**
  - draw line between each pair of a label's closest opposing label.  
 ### choosing k
  - ![[Pasted image 20240831235915.png#invert]]
  - low k: high variance (noise), overfit.          too high k: high bias, underfit 
### practical considerations
- *normalize* features to be on same scale ok as correlation preserved 
- Euclidean is **not scale invariant**: higher range features will dominate!! even if shouldn't (not strongly predictive of label) 
- **standardization**: mean=0, std=1:  **![[a7ff0b66a85a796814aa1e8a9d633cf1_MD5.png#invert]]**
	- for normal distributions w/out outlier sensitivity  
- **min**-**max**: range is \[0,1\] 		**![[229896462ee6cbf3b92d8fa256406013_MD5.png#invert]]**
	- for non-normal distribution w/ outlier sensitivity  
- **scalability**: bad at more dimensions, as data points more unique, less similar neighbors
- **tied** (k=even or equidistant neighbors): fall back to k-1, k-2, etc. (like n-grams)  
- **better implementation**? k-d trees or ball trees (group points into boxes first)  
	- **![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeV4yzrEBMxJaDjFOnMWetjChtHmZTJgNPmZ8DxlGBbx8gZO9o21F9SVmHG5vNi4rYE5aI_pF6p2HTLsnAK-yL4fqnncN5Xdudlr63vVA31mL1w4RmcfZLdTGziGTE5TMwQelK-OZIW9yfTlY8SDpLi0RM?key=pYbAc6IAUxOBBNv7wwwwJQ#invert)** too expensive to compute every individual pt distance to query pt
### CODE  
- see documentation for more hyperparameters: [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
- using [Iris DataSet](http://archive.ics.uci.edu/ml/datasets/Iris)
```
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df = pd.read_csv(filename, header=0)
X = df[['petal_length', 'petal_width']]
y = df['species'] 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

k=3
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train, y_train) 
prediction= model.predict(X_test) 

score = accuracy_score(y_test, prediction)
```

## Decision Trees

- [[Cheat Sheet Decision Trees.pdf]]
- **model-based**, splits into partitions for efficiency
- also works for classification (vote) & regression (mean/median of region)
- Goal: split dataset to minimize entropy/uncertainty and maximize info gain
- **Efficiency implementation**: `model.feature_importances_` shows feature contribution to decision tree (sorted by most cumulative information gain)
### Entropy [[statistics]]
- ![[Pasted image 20240901003941.png#invert]] with P(x) being ct-n-divide
- avg # bits of info needed to represent info. 
	- 50/50 coin toss: 1 bit. 70/30 coin: 0.88 bit 100/0 coin: 0 bits.
- **Mutual Information**  
    - H(x1|x2) \= **uncertainty/entropy** of x1 given x2
	    - 0 = given x2, 100% certain of x1
	    - 1: given x2, 0% certain of x1 (i.e. toss a coin)
    - **![[736d1bc21086a90dffed5cff30d91077_MD5.png#invert]]
    - ![[Pasted image 20240901023945.png#invert]]
		- H(y|parent): entropy before split
		- summation: for each new split regions c 
		- r(c): % of examples each split region is of parent
		- H(y|c): entropy of split region
    - ![[Pasted image 20240901004334.png#invert]] here, data sliced int left (84%) and right (16%), with new entropies both lower than before (info gained). 
	    - Example: `H(x1) = 1,  H(x1|x2) = 0.1`
	    - so **gained 0.9** information on x1 *after* learning x2
	- ![[Pasted image 20240901024919.png#invert]] can see how as you go down tree, probability of + label closer to 0 and 1 (more certain one way or another)
### Hyperparams
- **Leaf Sample Size**: min. # examples in each leaf split region:
- ![[Pasted image 20240901025627.png#invert]]
- **max depth**: max # splits we can make
- **min samples split:** min # samples in parent node required to split it
### CODE
- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) 
```
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df = pd.read_csv(filename, header=0)
X = df.drop(columns=['Churn'])
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

model = DecisionTreeClassifier(
			criterion="entropy" # default func: gini
			max_depth=16, 
			min_samples_split=128, 
			min_samples_leaf=64
		)
model.fit(X_train, y_train)
class_probabilities = model.predict_proba(X_test)[:,1] # prob is each class
y_predictions = model.predict(X_test) # directly predict class
acc_score = accuracy_score(y_test, y_predictions)

# plot 10 most predictive features on which examples were split: 
fi = model.feature_importances_
names_sorted = [x for _,x in sorted(zip(fi, X_train.columns.values), reverse = True)]
fig, ax = plt.subplots()
ax.bar(np.arange(10), sorted(model.feature_importances_, reverse=True)[:10], width = 0.35)
ax.set_xticks(np.arange(10))
ax.set_xticklabels(names_sorted[:10], rotation = 90)
plt.title('Feature importance from DT')
ax.set_ylabel('Normalized importance')
```


# Unit 3 Questions:
Answer the questions below about building and evaluating your models using algorithms such as decision trees and k-nearest neighbors.

## 1. Decision Tree Pros/Cons
 **Advantages:**
- can handle various data types
- very good at non-linear relationships 
- mimic human-making decision, clear, transparent, high interpretability
- non-parametric models, no assumptions on data distribution
- automatic feature selection in training (feature importance rankings for insight)
- handle missing values and outliers robustly
**Disadvantages:** 
- tendency to overfit training data, capturing noise, need to prune
- less predictive accuracy
- highly sensitive to small variations in training data, instability, less robust
- bias toward dominant classes in classification tasks with imbalanced data
- lack of smoothness in decision boundaries like regression 
- may struggle w/ complex, non-linear relationships 
- greedy nature, prioritizing immediate reduction in impurity not always optimal

## 2. KNN Pros/Cons
**Advantages:** 
- lazy/instance learning, no training period, directly utilizes historical data to predict. 
- highly intuitive and simple to understand, straightforward to implement like DT
- offers versatility, easily handling multi-class problems without extra effort 
- only two hyperparameter (k, dist) to tune
**Disadvantages:**
- does not perform well with large datasets, (dist btwn all examples) 
- The curse of dimensionality (less similar neighbors)
- sensitive to noisy and missing data
- requirement for proper feature scaling for accurate distance calculations 
- highly sensitive to outliers bc instance-based
- lacks inherent capabilities to handle missing values
- class inbalance may decrease performance drastically

## 3. KNN vs DT
- example factors: size of the training data, accuracy requirements, training time, linearity, number of features, supervised/unsupervised. 
- KNN must always have data to make predictions while decision trees construct a model. so slower prediction for KNNs
- Decision trees are more suitable for large or noisy datasets than KNN. 
- also useful for complex situations as more easy to interpret, i.e. subpop. hierarchies
- KNN for data w/ clear clusters already & meaningful distances in its feature vectors.  

## 4. Hyperparams?
- not learned directly from the training data but are set before training process
- control behavior and performance of machine learning algorithms. 
- Tuning hyperparameters can significantly impact performance and generalization ability 
- KNN:  
	- k  (the # of neighbors to consider) 
	- distance metric, (method to measure distances between data points)
- Decision trees
	- whether to prune to max depth.
	- minimum number of data points or samples required to be at a leaf node. 

## 5. Overfitting? Solutions? 
- when model learns to capture noise in the training data rather than  underlying pattern. 
- model performs well on training data but poorly on unseen or test data
- can perform pruning, removing parts of tree that do not provide significant predictive power. (max depth, min # samples required to split node, min # samples in leaf) 

## 6. Why Split Data?
- **training:** enable it to learn underlying patterns and relationships 
- **validation:** for evaluating model\'s performance after training, aiding in hyperparameter tuning 
- **test:** kept entirely separate to provide unbiased assessment of the final model performance
- **K-fold cross-validation** takes this concept even farther by training and testing the model\'s performance on multiple subsets of the data. This helps identify if the model is overfitting to specific training instances or features instead of just one division of the data into train and test subsets. 